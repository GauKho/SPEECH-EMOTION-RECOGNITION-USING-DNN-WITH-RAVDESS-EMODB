# Speech Emotion Recognition Using Deep Neural Networks
 
## 📌 Introduction

This project focuses on building a Speech Emotion Recognition (SER) system using Deep Neural Networks (DNN). The system aims to automatically identify human emotions from speech signals by learning discriminative acoustic features.

Two widely used benchmark datasets are employed in this project:

RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)

EMO-DB (Berlin Emotional Speech Database)

The project is designed for research and educational purposes and can be extended to real-world applications such as virtual assistants, human–computer interaction, and affective computing.

## 🎯 Objectives

Develop a Deep Neural Network model for speech emotion recognition

Extract effective acoustic features from raw audio signals

Train and evaluate the model on standard emotional speech datasets

Analyze and compare performance across different datasets

##📂 Datasets

### 1️⃣ RAVDESS

24 professional actors (12 male, 12 female), 8 emotion classes:

- Neutral
- Calm
- Happy
- Sad
- Angry
- Fearful
- Disgust
- Surprised

Audio format: .wav

Language: English

2️⃣ EMO-DB

10 actors (5 male, 5 female), 7 emotion classes:

- Neutral
- Happy
- Sad
- Angry
- Fear
- Disgust
- Boredom

Audio format: .wav

Language: German

⚠️ Note: Due to licensing restrictions, datasets must be downloaded manually from their official sources.



