# Speech Emotion Recognition Using Deep Neural Networks

## üìå Introduction
- This project focuses on building a **Speech Emotion Recognition (SER)** system using **Deep Neural Networks (DNN)**.
- The system automatically identifies human emotions from speech signals by learning discriminative acoustic features.
- Two widely used benchmark datasets are employed:
  - **RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)**
  - **EMO-DB (Berlin Emotional Speech Database)**
- The project is designed for:
  - Research and educational purposes
  - Real-world applications such as:
    - Virtual assistants
    - Human‚Äìcomputer interaction
    - Affective computing

---

## üéØ Objectives
- Develop a **Deep Neural Network (DNN)** model for speech emotion recognition
- Extract effective acoustic features from raw audio signals
- Train and evaluate the model on standard emotional speech datasets
- Analyze and compare performance across different datasets

---

## üìÇ Datasets

### 1Ô∏è‚É£ RAVDESS
- 24 professional actors:
  - 12 male
  - 12 female
- 8 emotion classes:
  - Neutral
  - Calm
  - Happy
  - Sad
  - Angry
  - Fearful
  - Disgust
  - Surprised
- Audio format: `.wav`
- Language: English

---

### 2Ô∏è‚É£ EMO-DB
- 10 actors:
  - 5 male
  - 5 female
- 7 emotion classes:
  - Neutral
  - Happy
  - Sad
  - Angry
  - Fear
  - Disgust
  - Boredom
- Audio format: `.wav`
- Language: German

---

‚ö†Ô∏è **Note:** Due to licensing restrictions, datasets must be downloaded manually from their official sources.

---

## üìä Results
- The proposed DNN-based Speech Emotion Recognition system achieves:
  - **Accuracy: over 90%**
- The results demonstrate:
  - The effectiveness of deep learning models for SER
  - Strong performance on benchmark emotional speech datasets
